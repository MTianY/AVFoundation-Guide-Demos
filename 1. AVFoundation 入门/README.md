### 一. AV Foundation 入门

- AVFoundation 的构建考虑到目前硬件环境和应用程序,其设计过程高度依赖多线程机制.充分利用了多核硬件的优势并大量使用 block 和 Grand Central Dispatch 机制将复杂的计算进程放在后台线程运行.
- 会自动提供硬件加速操作,确保在大部分设备上应用程序能以最佳性能运行.
- 同时也充分考虑了电量效率来满足诸如 iPhone 和 iPad 这类移动设备对电量控制的高要求.
- 此外从一开始改框架就是针对64位处理器设计的,可以发挥64位处理器的所有优势.


### 了解数字媒体

#### 1.什么是采样?

我们看到的信号标志和所听到的音乐都是通过模拟信号传递给我们的,我们的眼睛和耳朵的构造将这些信息转换为我们大脑能够解析出的电信号.现实生活中的信号是连续的,信号的频率和强度是在不断变化的.但是数字世界的信号是离散的,由1和0两个状态表示.要将模拟信号转换成我们能够存储并传输的数字信号,要经过模拟-数字转换过程,我们将这个过程称为`采样(Sampling)`.

#### 2.数字媒体采样

对媒体内容进行数字化主要有两种方式.

- 第一种: 时间采样
    - 这种方法捕捉一个信号周期内的变化.
    - 如在 iPhone 上记录一个音频备忘录时,在录制期间你所有的音高变化和声调变化都会被捕捉下来. 
- 第二种: 空间采样
    - 一般用在图片数字化和其他可视化媒体内容数字化的过程中.
    - 空间采样包含对一幅图片在一定分辨率之下捕捉其亮度和色度,进而创建由改图片的像素点数据所构成的数字化结果.

当对一段视频进行数字化时,这两种方式都可以使用,因为通常的视频信号既有空间属性又有时间属性.
我们开发者其实不需要对这两个采样过程中所用到的具体数字化信号处理原理有很深的研究.因为设备硬件可以帮助我们完成模拟信号到数字信号的转换,但是如果不了解这些采样过程的基本原理及之后的数字媒体内容的存储格式的话,在你进一步深入学习 AV Foundation 更高级、更有趣功能的时候会感到比较吃力.

#### 3.音频采样介绍

一般当我们听到一个人的声音、号角声或吉他的弹奏声,你真正听到的是声波通过一定介质传播过来的振动.
当我们记录一个声音时,比如注入钢琴或吉他等乐器所发出的声音,或捕捉其他环境的声音时,一般会使用麦克风设备.麦克风是将机械能量(声波)转换成电能量(电压信号)的转换设备.

**采样率**

- 音频数字化的过程包含一个编码方法,称为`线性脉冲编码调剂`(linear pulse-code modulation),比较常见的说法是 Linear PCM 或 LPCM. 这个过程采样或测量一个固定的音频信号,过程的周期率被称为`采样率`.
    - 下图是展示了在一秒内对信号进行7次采样及信号的数字化结果图. 

    ![](media/15317979863111/15318783756840.jpg)       
    
    - 通过图1.5可以看出.低采样率的数字信号版本无法很好的表现原始数据.播放这样的音频信息通常会用在点击或弹出等动作的声音情况下.问题是图1-5所示的采样频率尚无法准确的表示信号的原意.

- 我们提高采样频率.得到如图1-6所示

    ![](media/15317979863111/15318785892439.jpg)

    - 这个效果已经提升很多,但是还不能准确表示原始信号


- 通过以上两个示例我们可以推测出如果`不断的提高采样的频率`,我们就`有可能以数字化方式准确表现原始信号的信息`.

**尼奎斯特频率(Nyquist rate)**

- 20世纪30年代, Harry Nyquist 是贝尔实验室的一名工程师,他准确的捕捉到了一个特定频率,该频率为需要采样对象的最高频率的2倍.
    - 如:你需要捕捉的音频素材的最高频率为10kHz, 你所需要的采样率最起码为20kHz 才能得到较好的数字化效果.
    - 使用 CD 录制的音频采样率为44.1kHz, 这就意味着能捕捉到的最大频率为22.05kHz. 刚刚高过人耳能够识别的频率范围(20kHz).44.1kHz 的采样率可能还不能捕捉到初始资源中的所有频率范围,这意味着采样点可能会受到录制环境的干扰,因为其无法捕捉到 Abbey Road 会话的细微差别,不过对于人耳的听觉来说,这已经足够好了. 


**音频样本**

除了采样率外,数字音频采样的另一个重要方面是我们能够捕捉到什么精度的音频样本.

- 振幅在线性坐标系中进行测量,所以会有 Linear PCM 这个术语.
- 用来保存样本值的字节数定义了在线性维度上可行的离散度,同时这个信息也被称为音频的`位元深度`.
    - 为每个样本的整体量化分配过少的位结果信息会导致数字音频信号产生噪声和扭曲.
    - 使用位元深度为8的方法可以提供256个离散级别的数据,对于一些音频资源来说,这个级别的采样率已经足够了,但对于大部分音频内容来说还不够好.
    - CD 音质的位元深度位16.可以达到65536个离散级别.
    - 专业级别的音频录制环境的位元深度可以达到24或更高.

**原始,未压缩的数字呈现效果比较占用存储空间**

对信号进行数字化时,如果能够保留原始、未压缩的数字呈现效果,就是该媒体资源最纯粹的数字形式,但这样做需要大量的存储空间.
    
- 如:一个44.1kHz、 16位 LPCM 的音频文件每分钟可能要占用10MB 的空间.
- 要数字化一个含12首歌的唱片,每首歌曲时间大概为5分钟的话,共需要近600MB 的存储空间.

所以我们会看到在不经过压缩的`数字音频资源`会占用大量的存储空间,但是未压缩的视频文件呢? 

**视频文件在未压缩的时占用的存储空间是巨大的...**


视频文件由一系列称为"帧"的图片组成,在视频文件的时间轴线上每一帧都表示一个场景.要创建连续的运动画面,我们需要在短时间间隔内提供特定数量的帧.

- 视频文件一秒钟内所能展现的帧数称为视频的帧率,并用`FPS`作为单位进行测量.
- 常见的帧率: 24FPS, 25FPS, 30FPS
- 要计算未压缩的视频内容所需要的存储空间,我们首先需要确定每一个独立的帧有多大.
- 视频资源宽高比16:9.意思是每16个水平像素对应9个垂直像素.
- 如果对每个像素点使用8位的 RGB 色彩空间,这就意味着红色占8位,绿色占8位,蓝色占8位.下图住哪水了对于未压缩视频在30FPS 帧率的情况下,上述两个分辨率的存储空间需求(1280 * 720 和 1920 * 1080).

![](media/15317979863111/15318803726419.jpg)


- 经过上面图示可以看出这个存储空间占用率是相当大的,所以我们要找到一种方式缩小资源的尺寸.

### 数字媒体压缩

为缩小数字媒体文件的大小,我们需要对其使用压缩技术.
对数字媒体进行压缩可以大幅缩小文件的尺寸,但是通常会在资源的质量上有小幅可见的衰减.

#### 1.色彩二次抽样

视频数据时使用称之为 `Y'CbCr′`颜色模式的典型案例, `Y'CbCr′ `也常称为`YUV`.虽然 YUV 术语并不很准确,但是比读`Y-Prime-C-B-C-R`方便很多.
不过大部分开发者都更熟悉 `RGB` 颜色模式.即每个像素都是由红、绿、蓝三个颜色组合而成.
`Y'CbCr′ 或YUV`则使用色彩(颜色)通道 UV 替换了像素的`亮度通道 Y(亮度)`.

- 下图展示了一幅图片分离亮度和色彩通道后的效果

![](media/15317979863111/15318832438376.jpg)

- 可见所有细节都保存在亮度通道中,如果除去亮度,剩下的就是一幅灰度图片,我们再看整合的色彩通道中几乎所有的细节都丢失了.
- 这是因为我们的`眼睛对亮度的敏感度要高于颜色`.
- `聪明的工程师们认识到,我们可以大幅减少存储在每个像素中的颜色信息,而不至于图片的质量严重受损,这个减少颜色数据的过程就称为色彩二次抽样`.

当每次看到诸如摄像头规范和其他视频设备硬件或软件中提到的 `4:4:4 、 4:2:2及 4:2:0`时.这些值得含义就是这些设备所使用的色彩二次抽样的参数.

- 根据这些值按如下格式将亮度比例表示为色度值,这个格式写作`J:a:b`.具体含义如下:
    - J: 几个关联色块(一般是4个)中所包含的像素数
    - a: 用来保存位于第一行中的每个 J 像素的色度像素个数
    - b: 用来保存位于第二行中的每个 J 像素的附加像素个数.

- 为维持图片质量,每个像素点都需要有各自的亮度值,却不一定需要色度值.


#### 2.编解码器压缩

大部分音频和视频都是使用编解码器(codec)来压缩的,编解码器这个术语是由编码器/解码器结合简写得来的(encoder/decoder).编解码器使用高级压缩算法对需要保存或发送的音频或视频数据进行压缩和解码,同时它还可以将压缩文件解码成适合播放和编辑的媒体资源文件.

- 编解码器可以进行无损压缩也可以进行有损压缩.
- 无损压缩编解码器以一种可以完美重构解码的方式对媒体文件进行压缩,使其成为无论编辑还是发布都比较理想的文件,有时也会作为归档文件用.
- 有损编解码器就是在压缩过程中会有部分数据损失掉.编解码器为这一形式的压缩使用基于人类可感知的高级压缩算法.
    - 比如即使人类可以理论上听见介于20Hz 和20kHz 之间的声音,但我们可能真正敏感的频率区间是1kHz~5kHz, 我们对于频率的感知随着远离上述区间而逐渐减弱.这只是众多方法中的一个.
    - 但有损压缩的目的是使用`psycho-acoustic`或`psycho-visual`模式作为一种方法来减少媒体内容中的冗余数据.这样会使原文件质量的损耗达到最小.

#### 3.视频编解码器  

对于视频编解码而言, AV Foundation 提供有限的编解码器集合,只提供苹果公司认定的目前最主流的几种媒体类型的支持.具体对于视频文件来说,主要可以归结为`H.264`和`Apple ProRes`.

**H.264**

当我们需要对要发送的视频文件进行编码时, Henry Ford 曾经说过,只要是 H.264标准的文件, AVFoundation 都提供视频编解码器支持.

- 幸运的是行业内也广泛采用这个标准.
- H.264规范是 Motion Picture Experts Group(MPEG)所定义的 MPEG-4的一部分.
- H.264遵循早起的 MEPG-1和 MPEG-2标准,但是在以更低比特率得到更高图片质量方面有了长足进步,使其更好地用于流媒体文件和移动设备及视频摄像头.

H.264与其他形式 MPEG 压缩一样,通过以下两个维度缩小了视频文件的尺寸:

- 空间
    - 压缩独立视频帧,被称为`帧内压缩`.   
- 时间
    - 通过以组为单位的视频帧压缩冗余数据,这一过程称为`帧间压缩`. 

**帧内压缩**

- 通过消除包含在每个独立视频帧内的色彩及结构中的冗余信息来进行压缩,因此可在不降低图片质量的情况下尽可能缩小尺寸.
- 这类压缩同 JEPG 压缩的原理类似.
- 帧内压缩也可以作为有损压缩算法,但通常用于对原始图片的一部分进行处理以生成极高质量的照片.通过这一过程创建的帧称为 I-frames.

**帧间压缩**

- 很多帧被组合在一起作为一组图片(简称 GOP),对于 GOP 所存在的时间维度的冗余可以被消除.
    - 如:想象视频文件中的典型场景,就会有一些特定运动元素的概念,比如行驶的汽车或走路的行人,场景的背景环境通常是固定的.固定的背景环境就代表一个时间维度上的冗余,这个冗余就可以通过压缩方式进行消除.
    - 下图给出存储在 GOP 中的三个不同类型的帧

    ![](media/15317979863111/15318976249932.jpg)
 
    
- `I-frames`
    - 这些帧都是一些单独的帧或关键帧,包含创建完整图片需要的所有数据.
    - 每个 GOP 都正好有一个`I-frames`.由于它是一个独立帧,其尺寸是最大的,但也是解压最快的.  
- `P-frames`
    - 又称预测帧
    - 是从基于最近 I-frames 或 P-frames 的可预测的图片进行编码得到的.
    - P-frames 可以引用最近的预测 P-frames 或一组 I-frames.
    - 你将会经常看到这些被称为"reference frames"的帧,临近的P-frames 和 B-frames 都可以对其进行引用. 
- `B-frames`
    - 又称双向帧
    - 是基于使用之前和之后的帧信息进行编码后得到的帧.
    - 几乎不需要存储空间,但其解压过程会耗费较长时间,因为它依赖于周围其他的帧. 

**H.264支持编码视图,用来确定在整个编码过程中所使用的算法.共定义了3个高级标准**

- `Baseline`
    - 通常用于对移动设备的媒体内容进行处理,提供了`最低效`的压缩
    - 因此经过这个标准压缩后的文件仍较大
    - 但是这种方法也是最少计算强度的方法,因为他不支持 B-frames
    - 如果开发者的编译目标是比较久远的 iPhone 设备,如 iPhone 3GS, 可能需要用到 Baseline 标准. 
- `Main`
    - 计算强度比 Baseline 高.
    - 因为他使用的算法更多,但可以达到比较高的压缩率
- `High`
    - 会得到最高质量的压缩效果,但他也是3种方法中计算复杂度最高的,因为所有能用到的编码技术和算法几乎都用到了.

**Apple ProRes**

- AVFoundation 支持 Apple ProRes 编解码器的两种不同风格. Apple ProRes 被认为是一个中间件或中间层编解码器,他是为专业编辑和生产工作流服务.
- 独立于帧,意味着只有 I-frames 可以被使用,这就使其更适合用在内容编辑上.
- Apple ProRes 还使用可变比特率编码的方式来对复杂场景中的每一帧进行编码.
- ProRes 是有损编解码器,但是它具有最高的编解码质量.
- ProRes 编解码器只在 OS X上可用,如果开发者只针对 iOS 进行开发,只能使用 H.264.

#### 4.音频编解码器

只要是`Core Audio`框架支持的音频编解码,`AVFoundation`都可以支持,这意味着 AVFoundation 能够支持大量不同格式的资源.然而在不用线性 PCM 音频的情况下,更多的只能使用 AAC.

**AAC**

- 高级音频编码(AAC)是 H.264标准相应的音频处理方式,目前已成为音频流和下载的音频资源中最主流的编码方式.
- 比 MP3格式有显著的提升,可以在低比特率的前提下提供更高质量的音频,是在 Web 上发布和传播的音频格式中最为理想的.
- 没有来自证书和许可方面的限制.(这一限制曾经在 MP3上饱受诟病)
- AVFoundation 和 CoreAudio 提供对 MP3数据解码的支持,但是不支持对其进行编码.

### 容器格式

日常中在我们电脑上回看到各种不同类型的媒体文件.它们扩展名有各种结尾,如`.mov、.m4v、.mpg 和.m4a`等.我们通常将这些类型都认为是文件格式,但是其正确定义应该是这些类型都是`文件的容器格式(container format)`.

- 容器格式被认为是元文件格式.
- 每种格式都有一个规范用来确定文件的结构.

当开发者使用 AVFoundation 撰写代码时,将遇到两类主要的容器格式,分别为

- QuickTime
    - QuickTime 是苹果公司在更宏观 QuickTime 架构中定义的最常用格式.
    - 其具有非常高的可靠性并且是一种有着非常清晰定义的格式. 
- MPEG-4
    - MPEG-4 Part 14规范定义 MPEG-4(MP4)容器格式.
    - 这是从 QuickTime 规范中直接派生出来的一种行业标准格式,所以这两个规范在结构和功能方面非常类似.
    - MP4容器格式官方文件扩展名是`.mp4`




